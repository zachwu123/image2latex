{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preprocess"
      ],
      "metadata": {
        "id": "pscJEUF1ZV0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYe2KIOANpqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550675a9-ace1-4876-ee19-97d793526e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf /content/drive/MyDrive/301Project/off_image_test.tar.gz\n",
        "!tar -xzvf /content/drive/MyDrive/301Project/off_image_train.tar.gz"
      ],
      "metadata": {
        "id": "I1DiPaQHN0Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "image1 = Image.open(\"off_image_train/MfrDB1326_0.bmp\")\n",
        "plt.imshow(image1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "K13mDbh9N5Vv",
        "outputId": "3553f075-7b2c-4c62-ac61-9fa917bf3388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x79296814c4c0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, caption_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (string): Directory with all the images.\n",
        "            caption_file (string): Path to the text file with image captions.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.image_labels = self._load_labels(caption_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def _load_labels(self, caption_file):\n",
        "        # Load the labels from the file\n",
        "        image_labels = {}\n",
        "        with open(caption_file, 'r') as file:\n",
        "            for line in file:\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) != 2:\n",
        "                    print(f'illegal line: {line}')\n",
        "                    continue\n",
        "                image_labels[parts[0]] = parts[1]\n",
        "        return image_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image name and its corresponding label\n",
        "        image_name = list(self.image_labels.keys())[idx]\n",
        "        label = self.image_labels[image_name]\n",
        "\n",
        "        # Load image\n",
        "        image_path = os.path.join(self.image_dir, image_name + '_0.bmp')\n",
        "        image = Image.open(image_path).convert('L')\n",
        "\n",
        "        # Apply transformation\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert label to tensor or numerical format if required\n",
        "        # For example, using a label encoder or dictionary mapping\n",
        "\n",
        "        sample = {'image': image, 'label': label}\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "DPVx5X48N_AF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define the directory where the images are located\n",
        "image_dir = 'off_image_train'\n",
        "\n",
        "# Path to the caption file\n",
        "caption_file_path = '/content/drive/MyDrive/301Project/train_caption.txt'\n",
        "\n",
        "# Function to load image names and labels from the caption file\n",
        "def load_image_labels(caption_file_path):\n",
        "    image_labels = {}\n",
        "    with open(caption_file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                image_labels[parts[0]] = parts[1]\n",
        "    return image_labels\n",
        "\n",
        "# Function to display a random image and its label\n",
        "def display_random_image(image_dir, image_labels):\n",
        "    random_key = random.choice(list(image_labels.keys()))\n",
        "    image_path = os.path.join(image_dir, random_key + '_0.bmp')\n",
        "    image = Image.open(image_path)\n",
        "    label = image_labels[random_key]\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(label)\n",
        "    plt.axis('off')  # Hide the axis\n",
        "    plt.show()\n",
        "    return image, label\n",
        "\n",
        "# Load image names and labels\n",
        "image_labels = load_image_labels(caption_file_path)\n",
        "\n",
        "# Display a random image and its label\n",
        "random_image, random_label = display_random_image(image_dir, image_labels)\n",
        "random_label  # Returning the label as well to output it in the next cell"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "Rppo1RiyODCt",
        "outputId": "1d3527e3-d9f7-488d-a5cc-443212a8a6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'( m ) ^ { \\\\phi }'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_path = '/content/drive/MyDrive/301Project/test_caption.txt'\n",
        "test_labels = load_image_labels(test_file_path)"
      ],
      "metadata": {
        "id": "qh2fRKLDOcE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "#image_path = '/content/off_image_train/'\n",
        "image_path = '/content/off_image_test/'\n",
        "#outFile = 'offline-train.pkl'\n",
        "outFile = 'offline-test.pkl'\n",
        "\n",
        "features = {}\n",
        "channels = 1\n",
        "sentNum = 0\n",
        "\n",
        "#caption_file_path = '/content/drive/MyDrive/301Project/train_caption.txt'\n",
        "caption_file_path = '/content/drive/MyDrive/301Project/test_caption.txt'\n",
        "\n",
        "# Use 'with' statements to handle files\n",
        "with open(caption_file_path) as scpFile, open(outFile, 'wb') as oupFp_feature:\n",
        "    while True:\n",
        "        line = scpFile.readline().strip()  # remove the '\\r\\n'\n",
        "        if not line:\n",
        "            break\n",
        "        else:\n",
        "            key = line.split('\\t')[0]\n",
        "            image_file = image_path + key + '_' + str(0) + '.bmp'\n",
        "            im = imageio.imread(image_file)\n",
        "            mat = np.zeros([channels, im.shape[0], im.shape[1]], dtype='uint8')\n",
        "            for channel in range(channels):\n",
        "                image_file = image_path + key + '_' + str(channel) + '.bmp'\n",
        "                im = imageio.imread(image_file)\n",
        "                mat[channel, :, :] = im\n",
        "            sentNum += 1\n",
        "            features[key] = mat\n",
        "            if sentNum % 500 == 0:\n",
        "                print('process sentences ', sentNum)\n",
        "\n",
        "    print('load images done. sentence number ', sentNum)\n",
        "\n",
        "    # Pickle the 'features' dictionary using the highest protocol available\n",
        "    pkl.dump(features, oupFp_feature, protocol=pkl.HIGHEST_PROTOCOL)\n",
        "    print('save file done')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQbgCSWmOJA3",
        "outputId": "ebbb557f-5766-4883-bf7a-14bc371e04bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-60eaf5a74811>:28: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  im = imageio.imread(image_file)\n",
            "<ipython-input-7-60eaf5a74811>:32: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  im = imageio.imread(image_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "process sentences  500\n",
            "load images done. sentence number  986\n",
            "save file done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gen_voc(infile, vocfile):\n",
        "    vocab = set()\n",
        "    with open(infile, encoding='utf-8') as f:  # Specify encoding if needed\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) != 2:\n",
        "                print('illegal line:', line)\n",
        "                continue\n",
        "            (title, label) = parts\n",
        "            for w in label.split():\n",
        "                vocab.add(w)  # No need to check if in vocab due to the nature of a set\n",
        "\n",
        "    with open(vocfile, 'w', encoding='utf-8') as fout:  # Specify encoding if needed\n",
        "        for i, w in enumerate(sorted(vocab)):  # Sort vocab for consistent ordering\n",
        "            fout.write('{}\\t{}\\n'.format(w, i+1))\n",
        "        fout.write('<eol>\\t0\\n')  # Assuming you want to include <eol> in your vocab\n",
        "    print('vocab file generated')\n",
        "\n",
        "train_cap = '/content/drive/MyDrive/301Project/train_caption.txt'\n",
        "test_cap = '/content/drive/MyDrive/301Project/test_caption.txt'\n",
        "vocfile = 'vocabulary.txt'\n",
        "gen_voc(train_cap, vocfile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUufzxtIP-f1",
        "outputId": "d4cbacd6-4412-44b8-de88-2734e96cf4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab file generated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder (Resnet50)"
      ],
      "metadata": {
        "id": "6VHfbZ6ZZaQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    \"\"\"Preprocess the image to be ResNet-50 compatible\"\"\"\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)[:100]\n",
        "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
        "    return preprocess_input(img_array_expanded_dims)\n",
        "\n",
        "def extract_features(directory):\n",
        "    \"\"\"Extract features from all images in the given directory\"\"\"\n",
        "    model = ResNet50(weights='imagenet', include_top=False)\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)  # Get the last convolutional layer\n",
        "    features = {}\n",
        "    for img_name in os.listdir(directory)[:3000]:\n",
        "        # Ensure processing only .bmp files\n",
        "        if img_name.lower().endswith('.bmp'):\n",
        "            img_path = os.path.join(directory, img_name)\n",
        "            img_preprocessed = preprocess_image(img_path)\n",
        "            features_vector = model.predict(img_preprocessed)\n",
        "            features_vector = features_vector.flatten()  # Flatten the features to a vector\n",
        "            img_id = img_name.split('.')[0][:-2]  # Assuming img_name format is 'id.bmp'\n",
        "            features[img_id] = features_vector\n",
        "    return features\n",
        "\n",
        "train_features = extract_features('/content/off_image_train')"
      ],
      "metadata": {
        "id": "w9WxEpAcUeKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features_test(directory):\n",
        "    \"\"\"Extract features from all images in the given directory\"\"\"\n",
        "    model = ResNet50(weights='imagenet', include_top=False)\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)  # Get the last convolutional layer\n",
        "    features = {}\n",
        "    for img_name in os.listdir(directory)[:800]:\n",
        "        # Ensure processing only .bmp files\n",
        "        if img_name.lower().endswith('.bmp'):\n",
        "            img_path = os.path.join(directory, img_name)\n",
        "            img_preprocessed = preprocess_image(img_path)\n",
        "            features_vector = model.predict(img_preprocessed)\n",
        "            features_vector = features_vector.flatten()  # Flatten the features to a vector\n",
        "            img_id = img_name.split('.')[0][:-2]  # Assuming img_name format is 'id.bmp'\n",
        "            features[img_id] = features_vector\n",
        "    return features\n",
        "\n",
        "# Example usage:\n",
        "test_features = extract_features_test('/content/off_image_test')"
      ],
      "metadata": {
        "id": "D0cPb5g9N79L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features"
      ],
      "metadata": {
        "id": "uqYRQlaAqtVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder (Resnet50+positional encoding)"
      ],
      "metadata": {
        "id": "1QCaWOnKZ3_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Assuming TensorFlow 2.x; for TensorFlow 1.x, adjustments may be needed\n",
        "#https://github.com/guillaumegenthial/im2latex/blob/master/model/components/positional.py\n",
        "def add_timing_signal_nd(x, min_timescale=1.0, max_timescale=1.0e4):\n",
        "    static_shape = x.shape.as_list()\n",
        "    num_dims = len(static_shape) - 2\n",
        "    channels = x.shape[-1]\n",
        "    num_timescales = channels // (num_dims * 2)\n",
        "    log_timescale_increment = (\n",
        "        math.log(float(max_timescale) / float(min_timescale)) /\n",
        "        (float(num_timescales) - 1))\n",
        "    inv_timescales = min_timescale * tf.exp(\n",
        "        tf.range(num_timescales, dtype=tf.float32) * -log_timescale_increment)\n",
        "    for dim in range(num_dims):\n",
        "        length = x.shape[dim + 1]\n",
        "        position = tf.range(length, dtype=tf.float32)\n",
        "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
        "            inv_timescales, 0)\n",
        "        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
        "        prepad = dim * 2 * num_timescales\n",
        "        postpad = channels - (dim + 1) * 2 * num_timescales\n",
        "        signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n",
        "        for _ in range(1 + dim):\n",
        "            signal = tf.expand_dims(signal, 0)\n",
        "        for _ in range(num_dims - 1 - dim):\n",
        "            signal = tf.expand_dims(signal, -2)\n",
        "        x = x + signal\n",
        "    return x\n",
        "\n",
        "# Load ResNet50 model\n",
        "model = ResNet50(weights='imagenet', include_top=False)\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\n",
        "# Example usage in a function\n",
        "def extract_features_with_positional_encoding(directory, set):\n",
        "    features = {}\n",
        "    if set == 'train':\n",
        "      for img_name in os.listdir(directory)[0:3000]:\n",
        "          if img_name.lower().endswith('.bmp'):\n",
        "              img_path = os.path.join(directory, img_name)\n",
        "              img_preprocessed = preprocess_image(img_path)\n",
        "              cnn_output = model(img_preprocessed)\n",
        "              cnn_output_with_pos = add_timing_signal_nd(cnn_output)\n",
        "              # Additional processing such as flattening if needed\n",
        "              features_vector = cnn_output_with_pos.numpy().flatten()\n",
        "              img_id = img_name.split('.')[0]\n",
        "              features[img_id] = features_vector\n",
        "    elif set == 'test':\n",
        "      for img_name in os.listdir(directory)[0:500]:\n",
        "          if img_name.lower().endswith('.bmp'):\n",
        "              img_path = os.path.join(directory, img_name)\n",
        "              img_preprocessed = preprocess_image(img_path)\n",
        "              cnn_output = model(img_preprocessed)\n",
        "              cnn_output_with_pos = add_timing_signal_nd(cnn_output)\n",
        "              # Additional processing such as flattening if needed\n",
        "              features_vector = cnn_output_with_pos.numpy().flatten()\n",
        "              img_id = img_name.split('.')[0]\n",
        "              features[img_id] = features_vector\n",
        "    return features\n",
        "\n",
        "# Example usage:\n",
        "train_features = extract_features_with_positional_encoding('/content/off_image_train', 'train')\n",
        "test_features = extract_features_with_positional_encoding('/content/off_image_test', 'test')"
      ],
      "metadata": {
        "id": "QVD6qOW8Z9Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ruQOsPki1fq",
        "outputId": "cc5b9dc0-9fd3-4cdc-920e-90ac433e7935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder (DenseNet169+positional)"
      ],
      "metadata": {
        "id": "el94NcSHUVWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow.keras.applications.densenet import DenseNet169, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    \"\"\"Preprocess the image to be DenseNet-169 compatible\"\"\"\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
        "    return preprocess_input(img_array_expanded_dims)\n",
        "\n",
        "def add_timing_signal_nd(x, min_timescale=1.0, max_timescale=1.0e4):\n",
        "    static_shape = x.shape.as_list()\n",
        "    num_dims = len(static_shape) - 2\n",
        "    channels = x.shape[-1]\n",
        "    num_timescales = channels // (num_dims * 2)\n",
        "    log_timescale_increment = (\n",
        "        math.log(float(max_timescale) / float(min_timescale)) /\n",
        "        (float(num_timescales) - 1))\n",
        "    inv_timescales = min_timescale * tf.exp(\n",
        "        tf.range(num_timescales, dtype=tf.float32) * -log_timescale_increment)\n",
        "    for dim in range(num_dims):\n",
        "        length = x.shape[dim + 1]\n",
        "        position = tf.range(length, dtype=tf.float32)\n",
        "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(\n",
        "            inv_timescales, 0)\n",
        "        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
        "        prepad = dim * 2 * num_timescales\n",
        "        postpad = channels - (dim + 1) * 2 * num_timescales\n",
        "        signal = tf.pad(signal, [[0, 0], [prepad, postpad]])\n",
        "        for _ in range(1 + dim):\n",
        "            signal = tf.expand_dims(signal, 0)\n",
        "        for _ in range(num_dims - 1 - dim):\n",
        "            signal = tf.expand_dims(signal, -2)\n",
        "        x = x + signal\n",
        "    return x\n",
        "\n",
        "# Load DenseNet169 model\n",
        "model = DenseNet169(weights='imagenet', include_top=False)\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\n",
        "def extract_features_with_positional_encoding(directory, set):\n",
        "    features = {}\n",
        "    if set == 'train':\n",
        "      for img_name in os.listdir(directory)[0:3000]:\n",
        "          if img_name.lower().endswith('.bmp'):\n",
        "              img_path = os.path.join(directory, img_name)\n",
        "              img_preprocessed = preprocess_image(img_path)\n",
        "              cnn_output = model(img_preprocessed)\n",
        "              cnn_output_with_pos = add_timing_signal_nd(cnn_output)\n",
        "              features_vector = cnn_output_with_pos.numpy().flatten()\n",
        "              img_id = img_name.split('.')[0]\n",
        "              features[img_id] = features_vector\n",
        "    elif set == 'test':\n",
        "      for img_name in os.listdir(directory)[0:500]:\n",
        "          if img_name.lower().endswith('.bmp'):\n",
        "              img_path = os.path.join(directory, img_name)\n",
        "              img_preprocessed = preprocess_image(img_path)\n",
        "              cnn_output = model(img_preprocessed)\n",
        "              cnn_output_with_pos = add_timing_signal_nd(cnn_output)\n",
        "              features_vector = cnn_output_with_pos.numpy().flatten()\n",
        "              img_id = img_name.split('.')[0]\n",
        "              features[img_id] = features_vector\n",
        "    return features\n",
        "\n",
        "\n",
        "train_features = extract_features_with_positional_encoding('/content/off_image_train', 'train')\n",
        "test_features = extract_features_with_positional_encoding('/content/off_image_test', 'test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08h5p4cFUZQc",
        "outputId": "6125e18f-92a6-4542-a039-a567b487b3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "51877672/51877672 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_features['200922-949-31'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9FS2S-6YsdK",
        "outputId": "239c42d7-0ec1-46e3-9522-b6fb0bd69796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81536"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Captions Tokenization"
      ],
      "metadata": {
        "id": "TyCU-OAOZgd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load image names and labels from the caption file\n",
        "def load_captions(caption_file_path):\n",
        "    image_labels = {}\n",
        "    with open(caption_file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) == 2:\n",
        "                image_labels[parts[0]] = parts[1]\n",
        "    return image_labels\n",
        "\n",
        "train_captions = load_captions(train_cap)\n",
        "test_captions = load_captions(test_cap)"
      ],
      "metadata": {
        "id": "RzvHy8lZaOjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = {key[:-2]: value for key, value in train_features.items() if key.endswith('_0')}\n",
        "test_features = {key[:-2]: value for key, value in test_features.items() if key.endswith('_0')}"
      ],
      "metadata": {
        "id": "sMqKbaO6pFD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming large_dict is your larger dictionary and small_dict is your smaller dictionary.\n",
        "train_captions = {key: train_captions[key] for key in train_features.keys() if key in train_captions}\n",
        "test_captions = {key: test_captions[key] for key in test_features.keys() if key in test_captions}"
      ],
      "metadata": {
        "id": "rlqg07Y5kFDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_captions(captions_dict):\n",
        "    \"\"\"Preprocess captions by adding start and end tokens.\"\"\"\n",
        "    for image_id, caption in captions_dict.items():\n",
        "        # Add start and end tokens\n",
        "        captions_dict[image_id] = f'<start> {caption} <end>'\n",
        "    return list(captions_dict.values())\n",
        "\n",
        "train_captions_list = preprocess_captions(train_captions)"
      ],
      "metadata": {
        "id": "fRrmQYsZcPOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_captions_list"
      ],
      "metadata": {
        "id": "kKUfAO1N43JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the captions\n",
        "tokenizer = Tokenizer(filters='', split=' ', lower=False)\n",
        "tokenizer.fit_on_texts(train_captions_list)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Includes padding token\n",
        "\n",
        "# Convert captions to sequences\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions_list)\n",
        "\n",
        "# Determine the maximum sequence length\n",
        "max_length = max(len(seq) for seq in train_seqs)\n",
        "\n",
        "# Pad\n",
        "train_seqs = pad_sequences(train_seqs, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "BMW8IJ6fcQKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seqs[0]"
      ],
      "metadata": {
        "id": "cjKX_nmZg2mK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff274d34-9c53-4a55-8e1a-80acb7b76fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, 12,  5,  8,  9, 13,  7,  1, 47,  2, 75,  5,  8, 47,  9,  4,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder (LSTM)"
      ],
      "metadata": {
        "id": "IumiMOTnZn8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Add, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "import numpy as np\n",
        "\n",
        "# Define parameters\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Assuming tokenizer is defined\n",
        "max_length = max(len(seq) for seq in train_seqs)  # Assuming train_seqs is defined\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "\n",
        "# Encoder\n",
        "# `input_feature_shape` is the shape of your current feature vector\n",
        "inputs1 = Input(shape=(81536,))\n",
        "# Project feature vector back to desired shape, e.g., 2048\n",
        "fe1 = Dense(2048, activation='relu')(inputs1)\n",
        "fe2 = Dense(embedding_dim, activation='relu')(fe1)  # Output shape: (batch_size, 256)\n",
        "\n",
        "# Decoder\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3, _, _ = LSTM(units, return_sequences=False, return_state=True)(se2)  # Output shape: (batch_size, 512)\n",
        "\n",
        "# Decoder model, using Concatenate\n",
        "decoder1 = Concatenate()([fe2, se3])  # New shape: (batch_size, 256+512)\n",
        "decoder2 = Dense(embedding_dim, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "d76gz8Hcez6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65ff154-8554-413f-b5b1-3797f7ac4530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None, 98)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)        [(None, 81536)]              0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 98, 256)              28672     ['input_5[0][0]']             \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 2048)                 1669877   ['input_4[0][0]']             \n",
            "                                                          76                                      \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 98, 256)              0         ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 256)                  524544    ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, 512),                1574912   ['dropout_1[0][0]']           \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 768)                  0         ['dense_5[0][0]',             \n",
            " )                                                                   'lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 256)                  196864    ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 112)                  28784     ['dense_6[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 169341552 (645.99 MB)\n",
            "Trainable params: 169341552 (645.99 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decoder w/ gru"
      ],
      "metadata": {
        "id": "QrJhc3c-6nia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU, Dense, Input, Embedding, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# the input feature shape\n",
        "input_feature_shape = (57344,)  #!!! Make sure this is the intended shape!!! after any necessary preprocessing\n",
        "\n",
        "# Encoder\n",
        "inputs1 = Input(shape=input_feature_shape)\n",
        "fe1 = Dense(2048, activation='relu')(inputs1)\n",
        "fe2 = Dense(embedding_dim, activation='relu')(fe1)\n",
        "\n",
        "# Decoder\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3, _ = GRU(units, return_sequences=False, return_state=True)(se2)\n",
        "\n",
        "# Decoder model, using Concatenate\n",
        "decoder1 = Concatenate()([fe2, se3])\n",
        "decoder2 = Dense(embedding_dim, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# model.fit(train_gen, epochs=10, steps_per_epoch=steps_per_epoch, verbose=1)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "8LGhbSwV6pz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "def data_generator(features, sequences, max_length, vocab_size, batch_size=32):\n",
        "    X1, X2, y = [], [], []\n",
        "    n = 0\n",
        "    # Loop over the entire dataset continuously\n",
        "    while True:\n",
        "        for i, seq in enumerate(sequences):\n",
        "            # Increment the loop counter\n",
        "            n += 1\n",
        "            # Find the image ID and retrieve the corresponding feature\n",
        "            img_id = list(features.keys())[i]\n",
        "            feature = features[img_id]\n",
        "\n",
        "            # Split the sequence into multiple X, y pairs for training\n",
        "            for i in range(1, len(seq)):\n",
        "                # Split into input and output pair\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                # Pad input sequence\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                # Encode output sequence\n",
        "                out_seq = to_categorical(out_seq, num_classes=vocab_size)\n",
        "\n",
        "                # Store the sequences\n",
        "                X1.append(feature)\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "\n",
        "            # Yield the batch data\n",
        "            if n == batch_size:\n",
        "                yield [[np.array(X1), np.array(X2)], np.array(y)]\n",
        "                X1, X2, y = [], [], []\n",
        "                n = 0\n"
      ],
      "metadata": {
        "id": "injoqrVcfSkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example values, replace with your actual dataset's details\n",
        "batch_size = 16\n",
        "steps_per_epoch = len(train_seqs) // batch_size\n",
        "\n",
        "# Create the generator\n",
        "train_gen = data_generator(train_features, train_seqs, max_length, vocab_size, batch_size)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_gen, epochs=10, steps_per_epoch=steps_per_epoch, verbose=1)"
      ],
      "metadata": {
        "id": "ygs5UEcafVsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa46dab-3646-409d-bf1c-e32f7c09a3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "187/187 [==============================] - 86s 429ms/step - loss: 9.3692\n",
            "Epoch 2/10\n",
            "187/187 [==============================] - 81s 434ms/step - loss: 0.4384\n",
            "Epoch 3/10\n",
            "187/187 [==============================] - 81s 435ms/step - loss: 0.3843\n",
            "Epoch 4/10\n",
            "187/187 [==============================] - 81s 436ms/step - loss: 0.3404\n",
            "Epoch 5/10\n",
            "187/187 [==============================] - 81s 435ms/step - loss: 0.3001\n",
            "Epoch 6/10\n",
            "187/187 [==============================] - 81s 434ms/step - loss: 0.2627\n",
            "Epoch 7/10\n",
            "187/187 [==============================] - 82s 437ms/step - loss: 0.2313\n",
            "Epoch 8/10\n",
            "187/187 [==============================] - 81s 434ms/step - loss: 0.2088\n",
            "Epoch 9/10\n",
            "187/187 [==============================] - 81s 435ms/step - loss: 0.1915\n",
            "Epoch 10/10\n",
            "187/187 [==============================] - 81s 434ms/step - loss: 0.1792\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7927e038d660>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and Evaluate"
      ],
      "metadata": {
        "id": "YyG0nw5kZw1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latex_expression(model, image_feature, tokenizer, max_length):\n",
        "    # Start the sequence with a start token\n",
        "    in_text = '<start>'\n",
        "    # Iterate over the max length of the sequence\n",
        "    for i in range(max_length):\n",
        "        # Convert the current sequence to a tokenized form\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # Pad the sequence\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # Predict the next word (as a probability distribution over the vocab)\n",
        "        yhat = model.predict([np.array([image_feature]), sequence], verbose=0)\n",
        "        # Convert the probability distribution to a specific token\n",
        "        yhat = np.argmax(yhat, axis=-1)\n",
        "        # Map the token to a word\n",
        "        word = tokenizer.index_word.get(yhat[0], None)\n",
        "        # Break if we cannot map the token or if we reach the end token\n",
        "        if word is None or word == '<end>':\n",
        "            break\n",
        "        # Append to the input sequence\n",
        "        in_text += ' ' + word\n",
        "    # Remove the start token for the final output\n",
        "    final_sequence = in_text.replace('<start> ', '')\n",
        "    return final_sequence\n",
        "\n",
        "# Initialize the dictionary to store predicted labels\n",
        "predicted_labels = {}\n",
        "\n",
        "# Assuming `test_features` is a dict with ResNet-50 extracted features for test images\n",
        "for img_id, feature in test_features.items():\n",
        "    predicted_seq = generate_latex_expression(model, feature, tokenizer, max_length)\n",
        "    # Store the predicted sequence in the dictionary\n",
        "    predicted_labels[img_id] = predicted_seq\n",
        "    #print(f\"Predicted LaTeX for image {img_id}: {predicted_seq}\")\n"
      ],
      "metadata": {
        "id": "RZ_aoNpU7YOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels"
      ],
      "metadata": {
        "id": "0yw8ySJSYdM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image(image_dir, image_name):\n",
        "    image_path = os.path.join(image_dir, image_name + '_0.bmp')\n",
        "    image = Image.open(image_path)\n",
        "    #label = image_labels[image_name]\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(image_name)\n",
        "    plt.axis('off')  # Hide the axis\n",
        "    plt.show()\n",
        "    return image\n",
        "\n",
        "image_dir = 'off_image_test'\n",
        "display_image(image_dir, \"RIT_2014_307\")"
      ],
      "metadata": {
        "id": "XhieMht17tCy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "106750f2-0078-409a-aa53-0eb1691f6667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.BmpImagePlugin.BmpImageFile image mode=L size=77x69>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAE0AAABFCAAAAAAqMTQBAAABDklEQVR4nO2WSw7CMAxEY8T9r2wWtKqTzNSDSyWEMhsMws+f2G1aW1pa+nWZ9jfX/CmN+p8BH5dhUc9CBjwmq9TllkaxSmtatP+jydMbxGdRo71n3g7TGyQqlXpYIGwmNCd2orzSrcqDaTwGp1mAqQ8AaUIADJef0fbD85adgEDz4TPatQkhdaJe5rS9TvRtlLILkzdLrc/NHYT1sFaMAmkojwlyhkv6trmazb+dx2ytn67Ryfv1KJ5pCJSqpxk0MRfh59x4Evnu194LRsADTb3O6Vtfu2wxGlW/FaiMcU9NyIy3A+UGgVJH9bezgptyO3+AfUq7JEYrXHohrcZhtCu6nWatXO53c1taWrpDL41wNmP4vgoKAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Assuming you have ground truth expressions and predicted expressions\n",
        "references = [[['this', 'is', 'a', 'test']]]  # Ground truth, wrapped in two lists\n",
        "candidates = [['this', 'is', 'a', 'test']]  # Predicted\n",
        "\n",
        "score = corpus_bleu(references, candidates)\n",
        "print(f\"BLEU score: {score}\")"
      ],
      "metadata": {
        "id": "c_hY1g9A8fPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_keys_equality(true_labels_dict, predicted_sequences_dict):\n",
        "    # Extract sets of keys from both dictionaries\n",
        "    true_labels_keys = set(true_labels_dict.keys())\n",
        "    predicted_sequences_keys = set(predicted_sequences_dict.keys())\n",
        "\n",
        "    # Check if the sets of keys are the same\n",
        "    if true_labels_keys == predicted_sequences_keys:\n",
        "        print(\"The sets of keys in both dictionaries are the same.\")\n",
        "        return True\n",
        "    else:\n",
        "        # If they're not the same, find differences\n",
        "        missing_in_predicted = true_labels_keys - predicted_sequences_keys\n",
        "        missing_in_true_labels = predicted_sequences_keys - true_labels_keys\n",
        "        if missing_in_predicted:\n",
        "            print(\"Keys missing in predicted_sequences:\", missing_in_predicted)\n",
        "        if missing_in_true_labels:\n",
        "            print(\"Keys missing in true_labels:\", missing_in_true_labels)\n",
        "        return False\n",
        "\n",
        "\n",
        "# Call the function\n",
        "keys_are_equal = check_keys_equality(test_captions, predicted_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVe2axdX_o7M",
        "outputId": "9e5812e8-74a9-4dee-a10d-ea9dd78dbc55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sets of keys in both dictionaries are the same.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_latex(latex_str):\n",
        "    \"\"\"\n",
        "    Tokenizes a LaTeX string into a list of tokens.\n",
        "    \"\"\"\n",
        "    # Basic tokenization process; can be improved for more complex LaTeX structures\n",
        "    tokens = latex_str.replace('\\\\', ' \\\\').split()\n",
        "    return tokens\n",
        "\n",
        "def evaluate_predictions(true_labels_dict, predicted_sequences_dict):\n",
        "    \"\"\"\n",
        "    Evaluates the predicted LaTeX sequences against the true labels.\n",
        "    \"\"\"\n",
        "    partially_correct_count = 0\n",
        "    all_correct_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    # Iterate over the true labels\n",
        "    for img_id, true_label in true_labels_dict.items():\n",
        "        if img_id in predicted_sequences_dict:\n",
        "            predicted_sequence = tokenize_latex(predicted_sequences_dict[img_id])\n",
        "            true_sequence = tokenize_latex(true_label)\n",
        "\n",
        "            # Calculate the intersection and the minimum necessary for partial correctness\n",
        "            matching_tokens = set(predicted_sequence) & set(true_sequence)\n",
        "            total_tokens = len(set(true_sequence))\n",
        "            partial_threshold = max(1, round(total_tokens * 0.5))  # Ensure there's at least 1 token for partial match\n",
        "\n",
        "            if len(matching_tokens) >= partial_threshold:\n",
        "                partially_correct_count += 1\n",
        "                if len(matching_tokens) == total_tokens and len(predicted_sequence) == len(true_sequence):\n",
        "                    all_correct_count += 1\n",
        "\n",
        "            total_count += 1\n",
        "\n",
        "    # Calculate percentages\n",
        "    partially_correct_pct = (partially_correct_count / total_count) * 100 if total_count > 0 else 0\n",
        "    all_correct_pct = (all_correct_count / total_count) * 100 if total_count > 0 else 0\n",
        "\n",
        "    return partially_correct_pct, all_correct_pct\n",
        "\n",
        "# Example usage\n",
        "true_labels = {\n",
        "    # Your true labels here\n",
        "}\n",
        "\n",
        "predicted_sequences = {\n",
        "    # Your predicted sequences here\n",
        "}\n",
        "\n",
        "partially_correct_pct, all_correct_pct = evaluate_predictions(test_captions, predicted_labels)\n",
        "print(f\"Partially Correct: {partially_correct_pct:.2f}%\")\n",
        "print(f\"All Correct: {all_correct_pct:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoqzxnLE5hK1",
        "outputId": "a7977dfd-17dc-44b1-86ee-cf001cbdcac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partially Correct: 49.40%\n",
            "All Correct: 0.00%\n"
          ]
        }
      ]
    }
  ]
}